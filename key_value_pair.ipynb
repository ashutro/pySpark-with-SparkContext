{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNDfnvFUwGDt"
      },
      "outputs": [],
      "source": [
        "# Install PySpark library\n",
        "!pip install -qq pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Configure Spark\n",
        "conf = SparkConf().setAppName(\"KeyValueReduceExample\").setMaster(\"local\")  # Set application name and execution mode\n",
        "sc = SparkContext(conf=conf)  # Create a SparkContext"
      ],
      "metadata": {
        "id": "RQFMb0v-xAeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data in the form of key-value pairs\n",
        "data = [\n",
        "    (\"apple\", 5),\n",
        "    (\"banana\", 3),\n",
        "    (\"apple\", 2),\n",
        "    (\"banana\", 4),\n",
        "    (\"orange\", 6)\n",
        "]\n",
        "\n",
        "# Create an RDD from the data\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Transformation: Use reduceByKey to calculate the sum of values for each key\n",
        "sums_rdd = rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Action: Collect the results and print\n",
        "results = sums_rdd.collect()\n",
        "for key, value in results:\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "# Stop the Spark context\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "id": "oAvMImrjwwe4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}